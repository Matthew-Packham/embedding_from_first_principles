{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP with Context + Embeddings\n",
    "\n",
    "Here is a Summary of KP2 and KP3, which has been `torchified`. Note: Just Layers created, Need to bring model all together into a `Model` Class with `.forward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = open(\"names.txt\", \"r\").read().splitlines()\n",
    "names[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Build the vocab ###\n",
    "\n",
    "#find all characters in our dataset\n",
    "vocab = sorted(list(set(''.join(names))))\n",
    "vocab.insert(0, \".\")\n",
    "\n",
    "#create vocab mappings\n",
    "char_to_idx = {char: idx for idx, char in enumerate(vocab)}\n",
    "idx_to_char = {idx: char for idx, char in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ---> e | [0, 0, 0] ---> 5\n",
      "..e ---> m | [0, 0, 5] ---> 13\n",
      ".em ---> m | [0, 5, 13] ---> 13\n",
      "emm ---> a | [5, 13, 13] ---> 1\n",
      "mma ---> . | [13, 13, 1] ---> 0\n",
      "\n",
      "olivia\n",
      "... ---> o | [0, 0, 0] ---> 15\n",
      "..o ---> l | [0, 0, 15] ---> 12\n",
      ".ol ---> i | [0, 15, 12] ---> 9\n",
      "oli ---> v | [15, 12, 9] ---> 22\n",
      "liv ---> i | [12, 9, 22] ---> 9\n",
      "ivi ---> a | [9, 22, 9] ---> 1\n",
      "via ---> . | [22, 9, 1] ---> 0\n",
      "\n",
      "ava\n",
      "... ---> a | [0, 0, 0] ---> 1\n",
      "..a ---> v | [0, 0, 1] ---> 22\n",
      ".av ---> a | [0, 1, 22] ---> 1\n",
      "ava ---> . | [1, 22, 1] ---> 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "context_len = 3\n",
    "X, Y = [], []\n",
    "\n",
    "counter_for_show = 0\n",
    "names_for_show = 3\n",
    "for name in names:\n",
    "    #initalise context with \".\" characters (index 0)\n",
    "    context = [0] * context_len\n",
    "    \n",
    "    if counter_for_show < names_for_show:\n",
    "        print(name)\n",
    "    \n",
    "    for char in name + \".\": # add end character to the name\n",
    "        y = char_to_idx[char]\n",
    "        X.append(context)\n",
    "        Y.append(y)\n",
    "        \n",
    "        if counter_for_show < names_for_show:\n",
    "            print(f'{\"\".join(idx_to_char[idx] for idx in context)} ---> {idx_to_char[y]} | {context} ---> {y}')\n",
    "\n",
    "        #shift the context (like a rolling window)\n",
    "        context = context[1:] + [y]\n",
    "    \n",
    "    if counter_for_show < names_for_show:\n",
    "        print(end='\\n')\n",
    "    counter_for_show += 1\n",
    "\n",
    "#store as tensors\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([228146, 3]) with dtype: torch.int64\n",
      "torch.Size([228146]) with dtype: torch.int64\n"
     ]
    }
   ],
   "source": [
    "print(f\"{X.shape} with dtype: {X.dtype}\")\n",
    "print(f\"{Y.shape} with dtype: {Y.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Dataset consists of 228146 examples of context length 3. Each example represents the index into our vocab. Now lets split this into training, validation and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(names, context_len=3):\n",
    "    \"\"\"Function to create a dataset out of a list of names given to it\n",
    "\n",
    "    Args:\n",
    "        names (list)\n",
    "        context_len (int, optional):Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        tuple: X, Y\n",
    "    \"\"\"\n",
    "\n",
    "    X, Y = [], []\n",
    "    for name in names:\n",
    "        #initalise context with \".\" characters (index 0)\n",
    "        context = [0] * context_len\n",
    "        \n",
    "        for char in name + \".\": # add end character to the name\n",
    "            y = char_to_idx[char]\n",
    "            X.append(context)\n",
    "            Y.append(y)\n",
    "        \n",
    "            #shift the context (like a rolling window)\n",
    "            context = context[1:] + [y]\n",
    "\n",
    "    #store as tensors\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split simply by indexing\n",
    "import random \n",
    "random.shuffle(names)\n",
    "train_split = int(0.8*len(names)) #80% for train\n",
    "val_split = int(0.9*len(names)) #10% for each of val and test\n",
    "\n",
    "X_train, Y_train = create_dataset(names[:train_split])\n",
    "X_val, Y_val = create_dataset(names[train_split:val_split])\n",
    "X_test, Y_test = create_dataset(names[val_split:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182532, 3]) torch.Size([182532])\n",
      "torch.Size([22873, 3]) torch.Size([22873])\n",
      "torch.Size([22741, 3]) torch.Size([22741])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_val.shape, Y_val.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "\n",
    "    def __init__(self, in_feats, out_feats, bias=True):\n",
    "        \"\"\"\n",
    "        Our initalisation. This function runs first and only once.\n",
    "        Creates our Parameters (weights and bias) according to the shapes we define\n",
    "        \"\"\"\n",
    "        self.weights = torch.randn((in_feats, out_feats)) / (in_feats)**0.5 # kaiming init (the gain we add later)\n",
    "        # 1D Array. Broadcasting will add a first dimension => row vector. Then copied horizontally\n",
    "        self.bias = torch.zeros((out_feats)) if bias else None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Defines what the layer does.\n",
    "        \"\"\"\n",
    "        self.out = x @ self.weights\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.weights] + [] if self.bias is None else [self.bias]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = Linear(200, 100) #(200, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1614, -0.9978,  1.5736,  ..., -0.0795, -0.5924,  0.9335],\n",
       "        [ 0.8366, -0.8196,  1.7949,  ...,  0.3813,  0.2425,  0.9816],\n",
       "        [-0.6354, -1.7995, -0.0213,  ...,  0.4595, -1.0686, -0.2287],\n",
       "        ...,\n",
       "        [-0.0298,  0.3662, -2.3881,  ..., -0.4450, -1.2235, -0.2654],\n",
       "        [-1.3655,  0.4128, -0.8359,  ...,  0.1139, -1.1340, -0.8745],\n",
       "        [-1.0898, -0.9293,  0.4817,  ...,  1.1814, -1.2138,  1.7310]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll(torch.randn((32, 200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0.])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1614, -0.9978,  1.5736,  ..., -0.0795, -0.5924,  0.9335],\n",
       "        [ 0.8366, -0.8196,  1.7949,  ...,  0.3813,  0.2425,  0.9816],\n",
       "        [-0.6354, -1.7995, -0.0213,  ...,  0.4595, -1.0686, -0.2287],\n",
       "        ...,\n",
       "        [-0.0298,  0.3662, -2.3881,  ..., -0.4450, -1.2235, -0.2654],\n",
       "        [-1.3655,  0.4128, -0.8359,  ...,  0.1139, -1.1340, -0.8745],\n",
       "        [-1.0898, -0.9293,  0.4817,  ...,  1.1814, -1.2138,  1.7310]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchNorm:\n",
    "    \n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1):\n",
    "\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "\n",
    "        # -- Training Flag --\n",
    "        # if what happens in the forward pass in training is different from the forward pass in inference, then we need to seperate the logic (using this flag!)\n",
    "        self.training = True\n",
    "\n",
    "        #  -- Two sets of parameters --\n",
    "        # Learnable scale and shift parameters\n",
    "        self.scale = torch.ones((1, num_features))\n",
    "        self.shift = torch.zeros((1, num_features))\n",
    "        # Buffers (Running mean and std, calculated iteratively, NOT Learned)\n",
    "        self.running_mean = torch.tensor([0])\n",
    "        self.running_std = torch.tensor([1])\n",
    "\n",
    "\n",
    "    def __call__(self, x: torch.tensor):\n",
    "        \n",
    "        # -- Forward Pass --\n",
    "        if self.training:\n",
    "            # X has shape (batch_size, number_of_features)\n",
    "            batch_mean = x.mean(dim=0, keepdim=True) # mean over the batch for each neuron, keep as a row vector; shape=(1, num_of_feats)\n",
    "            batch_std = x.std(dim=0, keepdim=True)\n",
    "        else: # running inference\n",
    "            batch_mean = self.running_mean\n",
    "            batch_std = self.running_std\n",
    "        \n",
    "        #save as anatrribute so we can loo at some statistics later - NOT pytorch standard\n",
    "        self.out = self.scale * ((x - batch_mean) / (batch_std + self.eps)) + self.shift\n",
    "        \n",
    "        ## Update the Running mean and std\n",
    "        if self.training:\n",
    "            \n",
    "            with torch.no_grad(): # telling pytorch we will never call .backwards() on these variables (prevent pytorch building out a computation graph)\n",
    "                self.running_mean = (1 - self.momentum) * self.running_mean + self.momentum * batch_mean # what it was before plus a little bit from this batch\n",
    "                self.running_std = (1 - self.momentum) * self.running_std + self.momentum * batch_std\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.scale, self.shift]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn = BatchNorm(num_features=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "          1., 1.]]),\n",
       " tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "          0., 0., 0., 0., 0., 0., 0., 0.]])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh():\n",
    "    \n",
    "    #No Constructor needed as we have no parameters\n",
    "    \n",
    "    def __call__(self, x: torch.tensor):\n",
    "        #Again creating a class variable so we can examine later but this isnt standard\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6640)\n"
     ]
    }
   ],
   "source": [
    "t = Tanh()\n",
    "print(t(torch.tensor(0.8)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking layers\n",
    "\n",
    "##### Long winded explaination of where the input into the first layer comes from\n",
    "* Recall that an example in our dataset is a list of size `context_len`, representing the indicies of our vocab. e.g. `... ---> e | [0, 0, 0] ---> 5`. So an example has shape `(context_len, )` its a 1D array.\n",
    "* Next we send this 1D array through our embedding matrix, which has shape `(vocab_len, emb_dim)`. So our input into the first layer of our network is `(1, context_len, emb_dim)`. This maes sense, we have three indices (the context len, e.g. [0, 0, 0]), each of which index's into our embedding matrix (e.g. C[0] will give (1, emb_dim) - but we have three of them, which is the context len, so we get C[[0, 0, 0]] is (3, emb_dim)).\n",
    "* next we take a batch of examples, rather than just one, so we get (`(batch_dim, context_len, emb_dim)`) - we have say 32 examples of an example which is a list of three indicies, for each of these indicies we index (i.e. select a row of) a matrix which has emb_dim number of columns\n",
    "* Finally, we view this Tensor as  `(batch_dim, context_len*emb_dim)` So each example (which is a list of 3 indices of our vocab) we flatten them into one long vector containing each of the indices embedding as one vector if C[0] is [2, 1] (i.e. emb_dim=2), then C[[0, 0, 0]] would be [[2, 1], [2, 1], [2, 1]] (shape: (3, 2)) which we now flatten to be [2, 1, 2, 1, 2, 1] (shape (6,) 1D array). We do this firstly because MLP expect 2D arrays (not 3D). secondly in a character-level language model we'd like to process the whole **context information all at once** (rather than seperately). Finally, by flattening, we allow the MLP to learn patterns across both the context and embedding dimensions **simultaneously** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_len=27\n",
    "context_len=3\n",
    "emb_dim=7\n",
    "batch_dim=64\n",
    "\n",
    "C = torch.randn((vocab_len, emb_dim))\n",
    "\n",
    "## -- Pre Processing --\n",
    "# Examples:                            (batch_dim, context_len)\n",
    "# Embed Examples:                      (batch_dim, context_len, emb_dim)\n",
    "# Flatten Examples:                    (batch_dim, context_len*emb_dim)\n",
    "# 1st Layer...\n",
    "layers = [\n",
    "    Linear(in_feats=context_len*emb_dim, out_feats=200), # Shape of weight matrix to give an output: (batch_dim, num_neurons_1st_layer)\n",
    "    BatchNorm(num_features=200),\n",
    "    Tanh(),\n",
    "    Linear(200, vocab_len)\n",
    "]\n",
    "\n",
    "# Apply the (5/3) part of the hamming!\n",
    "for layer in layers:\n",
    "    if isinstance(layer, Linear):\n",
    "        layer.weights *= (5/3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What `in_feats` and `out_feats` represent\n",
    "Its really important to understand what we mean by `in_feats` and `out_feats` in the Linear Layer. They define the shape of the weight matrix. They do **not** represent the shape of the output of the layer! The weigh matrix has shape: `(in_feats, out_feats)`. Throughout the neural network the `batch_dim` will remain the first dim of any output of a layer. Recall `(m, n)x(n, p)=(m, p)`. Then `n=in_feats` and `p=out_feats`( with `m=batch_dim`). Its what dimension the weight matrix should be. The reason theyre named as `in_feats` and `out_feats` is because if we consider the 2nd dimension (the columns) then second dim of the input into the layer is `in_feats` and the second dim of the output of tae layer is `out_feats` (usually the number of neurons in the layer). the first dim is always `batch_dim` remember."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [C] + [p for layer in layers for p in layer.parameters()]\n",
    "\n",
    "# each parameter is a tensor which we want to learn ==> so requires gradient!\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training ###\n",
    "\n",
    "max_steps = 100_000\n",
    "\n",
    "\n",
    "for i in range(max_steps):\n",
    "    \n",
    "    #create a batch\n",
    "    batch_idx = torch.randint(0, X_train.shape[0], size=(batch_dim,)) # 1D array of indices: (batch_dim,)\n",
    "    batch = X_train[batch_idx] # (batch_dim, context_len)\n",
    "    \n",
    "    ## -------------- FORWARD PASS ---------------- ##\n",
    "    #Embbed\n",
    "    emb=C[batch] # (batch_dim, context_len, emb_dim)\n",
    "    \n",
    "    #Flatten\n",
    "    x=emb.view(-1, context_len*emb_dim) # (batch_dim, context_len*emb_dim)\n",
    "    \n",
    "    for layer in layers:\n",
    "        x = layer(x) #We are calling each layer on the output from the previous layer x\n",
    "    \n",
    "    loss = F.cross_entropy(x, Y_train[batch_idx])\n",
    "    \n",
    "    break\n",
    "    ## ------------ BACKWARD PASS ------------------ ##\n",
    "    \n",
    "    # for layer in layers:\n",
    "    #     layer.out.retain_grad() # In Pytorch these are thrown away by autograd after they are computed to save memory - but we need to keep them\n",
    "    \n",
    "    # for p in parameters:\n",
    "    #     p.grad = None\n",
    "    # loss.backward()\n",
    "    \n",
    "    # ## ----------- UPDATE -------------------------- ##\n",
    "    # if i <= 100_000:\n",
    "    #     lr=0.1\n",
    "    # elif ((i>150_000) & (i<180_000)):\n",
    "    #     lr=0.01\n",
    "    # else:\n",
    "    #     lr=0.001\n",
    "    \n",
    "    # for p in parameters:\n",
    "    #     p.data += -lr * p.grad\n",
    "    \n",
    "    \n",
    "    # ## Track progress ...\n",
    "    # if i % 10_000 == 0:\n",
    "    #     print(f\"{loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\matth\\AppData\\Local\\Temp\\ipykernel_45120\\1160230981.py:1: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  layers[-1].out.grad\n"
     ]
    }
   ],
   "source": [
    "layers[-1].out.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9036\n",
      "2.4064\n",
      "2.6666\n",
      "2.5221\n",
      "2.6317\n",
      "2.1978\n",
      "2.3304\n",
      "2.4800\n",
      "2.5238\n",
      "2.4132\n",
      "2.6700\n",
      "2.1960\n",
      "2.4214\n",
      "2.5304\n",
      "2.2663\n",
      "2.2792\n",
      "2.2229\n",
      "2.7139\n",
      "2.3502\n",
      "2.3517\n",
      "2.2220\n",
      "2.1882\n",
      "2.3404\n",
      "2.1929\n",
      "2.2518\n"
     ]
    }
   ],
   "source": [
    "### Training ###\n",
    "\n",
    "max_steps = 250_000\n",
    "\n",
    "\n",
    "for i in range(max_steps):\n",
    "    \n",
    "    #create a batch\n",
    "    batch_idx = torch.randint(0, X_train.shape[0], size=(batch_dim,)) # 1D array of indices: (batch_dim,)\n",
    "    batch = X_train[batch_idx] # (batch_dim, context_len)\n",
    "    \n",
    "    ## -------------- FORWARD PASS ---------------- ##\n",
    "    #Embbed\n",
    "    emb=C[batch] # (batch_dim, context_len, emb_dim)\n",
    "    \n",
    "    #Flatten\n",
    "    x=emb.view(-1, context_len*emb_dim) # (batch_dim, context_len*emb_dim)\n",
    "    \n",
    "    for layer in layers:\n",
    "        x = layer(x) #We are calling each layer on the output from the previous layer x\n",
    "    \n",
    "    loss = F.cross_entropy(x, Y_train[batch_idx])\n",
    "    \n",
    "    # ------------ BACKWARD PASS ------------------ ##\n",
    "    \n",
    "    # In Pytorch Non-leaf nodes in the computational graph are thrown away by autograd after they are computed and used in the backwards pass\n",
    "    # This is to save memory. But we need to keep them\n",
    "    for layer in layers:\n",
    "        layer.out.retain_grad() \n",
    "    \n",
    "    # remove the old gradients from the previous backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    ## ----------- UPDATE -------------------------- ##\n",
    "    if i <= 100_000:\n",
    "        lr=0.1\n",
    "    elif ((i>150_000) & (i<180_000)):\n",
    "        lr=0.01\n",
    "    else:\n",
    "        lr=0.001\n",
    "    \n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "    \n",
    "    \n",
    "    ## Track progress ...\n",
    "    if i % 10_000 == 0:\n",
    "        print(f\"{loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def loss_on_split(split: str):\n",
    "    x, y = {\n",
    "        \"train\": (X_train, Y_train),\n",
    "        \"test\": (X_test, Y_test),\n",
    "        \"val\": (X_val, Y_val)\n",
    "    }[split]\n",
    "    \n",
    "    \n",
    "    #run forward pass and get loss\n",
    "    emb = C[x]\n",
    "    h = emb.view(-1, context_len*emb_dim) \n",
    "    \n",
    "    for layer in layers:\n",
    "        if isinstance(layer, BatchNorm):\n",
    "            layer.training = False\n",
    "        h = layer(h)\n",
    "    \n",
    "    loss = F.cross_entropy(h, y)\n",
    "    print(f\"{split} loss: {loss.item():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 2.391\n",
      "test loss: 2.390\n"
     ]
    }
   ],
   "source": [
    "loss_on_split(\"train\")\n",
    "loss_on_split(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "menl.\n",
      "madi.\n",
      "zallianaylan.\n",
      "ano.\n",
      "alanrerish.\n",
      "merynniaton.\n",
      "cesa.\n",
      "aagielenare.\n",
      "stcyu.\n",
      "eryaaowyla.\n"
     ]
    }
   ],
   "source": [
    "generated_chars = []\n",
    "NUMBER_OF_NAMES = 10\n",
    "\n",
    "for _ in range(NUMBER_OF_NAMES):\n",
    "    #start with ...\n",
    "    context = [0] * 3\n",
    "    \n",
    "    while True:\n",
    "        #send it through the forward pass\n",
    "        emb = C[context] #(1_example, context_len, emb_dim) = (1, 3, 10)\n",
    "        h = emb.view(-1, context_len*emb_dim) \n",
    "    \n",
    "        for layer in layers:\n",
    "            # So batch norm uses the running means for inference (rather than over the batch)\n",
    "            if isinstance(layer, BatchNorm):\n",
    "                layer.training = False\n",
    "            h = layer(h)\n",
    "        #now we create the prob dist \n",
    "        prob_dist = F.softmax(h, dim=1) #logits is a row vector, so calculate over the row (i.e. across dim 1 which the columns across the columns gives over the row)\n",
    "\n",
    "        #now we sample from the dist (a multinomal will do that for us)\n",
    "        idx = torch.multinomial(prob_dist, 1).item()\n",
    "        generated_chars.append(idx_to_char[idx])\n",
    "\n",
    "        if idx == 0:\n",
    "            break\n",
    "\n",
    "        context = context[1:] + [idx]\n",
    "\n",
    "    print(\"\".join(generated_chars))\n",
    "    #reset\n",
    "    generated_chars=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emb-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
